# LLM Implementation envs
# The following are just examples of the adapter implementation, you can have completely different envs
AZURE_LARGE_LLM_MODEL="gpt-4o"
AZURE_LARGE_LLM_API_VERSION="2024-12-01-preview"
AZURE_LARGE_LLM_ENDPOINT="https://yourproject.openai.azure.com"
AZURE_LLM_SUBSCRIPTION_KEY="xyz"
GCP_EXTRA_SMALL_LLM_MODEL="gemini-2.5-flash-lite"
GCP_SMALL_LLM_MODEL="gemini-2.5-flash"
GCP_PROJECT_ID="your-project-id"
GCP_CREDENTIALS_PATH=".path-to-credentials-file.json"

# Embedding envs
# The following are just examples of the adapter implementation, you can have completely different envs
AZURE_EMBEDDING_FULL_ENDPOINT="https://yourproject.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2023-05-15"
AZURE_EMBEDDING_KEY="xyz"
EMBEDDINGS_SMALL_MODEL="paraphrase-multilingual-MiniLM-L12-v2"

# Cache envs
# The following are just examples of the adapter implementation, you can have completely different envs
REDIS_HOST="localhost"
REDIS_PORT=6379

# Worker envs
# Can be whatever you prefer
# CELERY_BACKEND="rabbitmq"
CELERY_BACKEND="redis"

# GraphDB envs
# The following are just examples of the adapter implementation, you can have completely different envs
NEO4J_HOST="localhost"
NEO4J_PORT=7687
NEO4J_USERNAME="neo4j"
NEO4J_PASSWORD="your_password"

# VectorDB envs
# The following are just examples of the adapter implementation, you can have completely different envs
MILVUS_HOST="localhost"
MILVUS_PORT=19530

# DataDB envs
# The following are just examples of the adapter implementation, you can have completely different envs
MONGO_HOST="localhost"
MONGO_PORT=27017
MONGO_USERNAME="root"
MONGO_PASSWORD="password"

# Auth
# Used to handle authentication
BRAINPAT_TOKEN="your_token"

# MultiBrain
# Choose to allow or block automatic creation of new brains on requests with non existing new brain_ids
BRAIN_CREATION_ALLOWED="true"
# Choose whether to fallback to default brain if not provided
DEFAULT_BRAIN_FALLBACK="true"

# GCP
# Used on the oss project for the small LLM but can be changed to any other model
GCP_EXTRA_SMALL_LLM_MODEL="gemini-2.5-flash-lite"
GCP_SMALL_LLM_MODEL="gemini-3-flash-preview"
GCP_PROJECT_ID="your-project-id"
GCP_CREDENTIALS_PATH=".path-to-credentials-file.json"

# Celery
# Used to run the worker tasks
CELERY_WORKER_CONCURRENCY=4

# Pricing
# Used to calculate the pricing of the LLM calls
INPUT_TOKEN_PRICE=0.0000005
OUTPUT_TOKEN_PRICE=0.000003

# Graph Consolidator
# Used to consolidate the graph after ingestion of new data with a high level overview of the graph
RUN_GRAPH_CONSOLIDATOR="true"

# DocParser
# Used to parse the documents into markdown text
# and by forwarding the webhook (our app host) and the task identifier,
# we can be called back from the DocParser API to start the ingestion process
# with the created markdown text.
DOCPARSER_ENDPOINT="https://docparser.lumen-labs.ai"
APP_HOST="https://your-app-host.com/ingest"
DOCPARSER_TOKEN="your-docparser-token"

# Spacy
# Used to keep the spacy models in memory or reinitialize them on each request
SPACY_KEEP_MODELS_IN_MEMORY="true"